%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CrossWalk: Learning Cross-Domain Subgraph Representations}

\begin{document}

\twocolumn[
\icmltitle{CrossWalk: Learning Cross-Domain Subgraph Representations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Devin de Hueck}{red}
\icmlauthor{Frido Pokorny}{red}
\end{icmlauthorlist}

\icmlaffiliation{red}{Office of the CTO, Red Hat Inc}

\icmlcorrespondingauthor{Devin de Hueck}{d.dehueck@gmail.com}
\icmlcorrespondingauthor{Frido Pokorny}{fpokorny@redhat.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Graphs are a wonderful way to interpret data as they capture many complex relationships. However this data may be spread across domains, e.g. a python package can be characterized by a language graph, a python code graph, and a dependency graph. This same problem emerges across a wide variety of data-hungry industries where no single domain of data is sufficient to represent the entity being sought after i.e. a python package, a user in social media, or a web page. In this paper we help address this problem with a two-fold contribution: (1) We introduce the PyPI Graph Dataset that includes three graph data domains. (2) We benchmark this dataset with a novel method that can cross domains that we coin CrossWalk.
\end{abstract}

\section{Introduction}
The Python Package Index (PyPI) is home to over one-hundred and twenty-thousand python packages (Bommarito et al. 2019). Finding similar packages in terms of use-cases, features, and performance can be very difficult without specific domain knowledge of a task and the python ecosystem. 

One of deep learning's most fundamental advances is the ability to learn meaningful representations from raw data. The PyPI dataset presented in this is unique due to the multi-domain nature of its data which is all unified under the idea of a python package. That is to say, every python package may have a textual description, a place in a large dependency graph, and the python code itself. Each of these domains have been researched independently. Document representations have been a long researched area within NLP, graph representations are a hot area of research, code representations are a newer but largely unexplored area of representation learning. This dataset not only provides a new context to further explore these domains, but allows for researchers to explore the interaction and generalization of methods that may apply across all of these domains. 

One can imagine that a sentence can be thought of a random walk through a language graph thus, the strong performance of word2vec \cite{word2vec} and other neural probabilistic language models \cite{bengio2003neural} can be applied in an effective manner on arbitrary graphs to learn node representations \cite{deepwalk}. We argue that this analogy can and should be pushed further in the pursuit of building general cross-domain methods.

\section{Related Work}

To the best of our knowledge there is only one other paper that directly addresses the issue of cross-domain representations in the space of graph neural networks (GNNs). \cite{ouyang2019learning} introduces Deep Multi-Graph Embedding (DMGE) which relies on a multiple-gradient descent optimizer to balance learning across domains. 

Not only do we learn cross-domain representations we also learn a representation for subgraph. Sub2Vec as detailed in \cite{adhikari2017distributed} builds on the formalization in \cite{deepwalk} which itself extends the skip-gram model \cite{word2vec} to arbitrary graphs.

\section{The Dataset}

\begin{table}[t]
\caption{Summary of PyPI Graph Dataset}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Domain & \# Nodes & \# Edges\\
\midrule
Language     & 166,987 & & \\
Python Code  & & & \\
Dependencies & 95,951 & 1,509,378 & \\
Total        & 199,234 & & \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.15in
\end{table}

Within the PyPI dataset we interpret each package as a graph consisting of a subgraph for each domain. As shown in [ADD A FIGURE AND REFERENCE HERE] a package graph consists of a language graph, a dependency graph, and a python code graph. The following sections detail how we interpret and construct each subgraph.

\subsection{The Language Graph}

The language graph consists of either the package's GitHub README file or its PyPI description (whichever is longer). If we were to learn a python package representation from just this graph we would end up with a reasonable measure of package similarity as a package creator wants to explain what the package features. However, we reiterate that this one domain is a part to the whole concept that is a python package. [NEED TRANSITION]. We discuss the following methods that learn document representations: TF-IDF and Doc2Vec \cite{doc2vec}.

 TF-IDF builds vectors of length of a keyword dictionary. At the corresponding keyword’s index there is a value in the range [0, 1] indicating the relative importance of the term to that python package’s documentation. The main advantage of this approach is also the interpretability of the calculated vectors, but also encodes more information than the Binary Keyword approach. These vectors were calculated with sklearn’s TFIDFVectorizer class.
 
 Doc2Vec \cite{doc2vec} learns a dense representation of documents alongside a vocabulary. This extends the word2vec \cite{word2vec} approach to documents. This approach has a much better memory requirement as we don’t need each vector to be the length of a keyword dictionary. These word2vec type approaches are well known for encoding the semantic relationships of language.

\subsection{The Direct Dependency Graph}

\subsection{The Code Graph}

\subsection{Data Collection}


\section{CrossWalk}
Our maximum likelihood w.r.t the parameters of our model across all domains present becomes:
\begin{equation}
\arg\max_{\theta} \prod_{i}^{D}\prod_{t}^{V^{(i)}}P(v^{(i)}_{t} | v^{(i)}_{-w:w}, \mathbf{d}; \theta)
\end{equation}
Where $D$ is the set of all domains, $v^{(i)}_{t}$ is the $t$th vertex in the $i$th domain, and $\mathbf{d}$ is the document vector.

\section{Representation Evaluation}

\subsection{Topic Classification}
In order to evaluate the learned python package embeddings we use an extrinsic evaluation task. We define this task as the classification of python packages into different categories. Github allows for users to label their projects under various tags - as a vast majority of PyPI packages link to a github we were able to extract a subset of python packages that are labeled by the creator into these categories. Following the example of Perozzi et al. (maybe add others?) we use a logistic regression and a shallow neural network to classify the learned representations.

This task empirically shows that no single domain is sufficient to capture a complete representation of a python package.

\subsection{Link Prediction}

\subsection{Topic Generation}

\section{Conclusion}





% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{doc2vec}

\bibliography{example_paper}
\bibliographystyle{icml2019}

\end{document}
