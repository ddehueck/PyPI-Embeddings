%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learning One Cross-Domain Graph Representation with PyPI Data}

\begin{document}

\twocolumn[
\icmltitle{Learning One Cross-Domain Graph Representation with PyPI Data}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Devin de Hueck}{red}
\icmlauthor{Frido Pokorny}{red}
\end{icmlauthorlist}

\icmlaffiliation{red}{Office of the CTO, Red Hat Inc}

\icmlcorrespondingauthor{Devin de Hueck}{d.dehueck@gmail.com}
\icmlcorrespondingauthor{Frido Pokorny}{fpokorny@redhat.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
To understand a new dataset a practitioner may turn to sums, buckets, and averages to quantity different properties in the data. Traditionally, modern machine learning does not help in this situation - only in matters when one wants to predict. In this case study we present a careful analysis of a data space so large and specialized that traditional statics cannot properly characterize it's properties. We apply both traditional and more recent, advanced techniques to develop an embedding space for python packages hosted on PyPi. To evaluate these results we develop a task consisting of finding “package neighbors” determined by python developers. With this evaluation task, the multiple methods are compared and we recommend (make recommendation here), along with the 
\end{abstract}

\section{Introduction}
The Python Package Index (PyPI) is home to over one-hundred and twenty-thousand python packages (Bommarito et al. 2019). Finding similar packages in terms of use-cases, features, and performance can be very difficult without specific domain knowledge of a task and the Python ecosystem. Motivated by this we build multiple embedding spaces of python packages based on the documentation and/or graphical structure of direct dependencies.

By using the documentation of python packages this problem is framed as a natural language processing task. We take advantage of this and apply traditional NLP techniques as well as more advanced recent techniques like doc2vec (Le et al. 2014) and lda2vec (Moody 2016). 

However building an embedding space for a data such as python packages introduces a problem: How does one evaluate such a space when there are no experts that can evaluate a potential solution due to the extreme nichety of the space. E.g. With word embeddings any proficient speaker of the language can reasonably evaluate if the nearest-neighbors of an embedding are relevant. With an extremely niche space like python packages expert python developers may be able to evaluate a subset of the space (e.g. python web developers could indicate which packages are similar to a package like flask). To mitigate this problem we plan to sample these subsets of Python experts to get list of potential package neighbors that would be able to indicate the correctness of the resulting embedding space.


\section{The Dataset}

\subsection{Data Collection}

\section{Learning Python Package Representations}

One of Deep Learning's most relevant advances is the ability to learn meaningful representations from raw data. The PyPI dataset presented in this is unique due to the multi-domain nature of its data all unified under the idea of a python package. That is to say, every python package may have a textual description, a place in a large dependency graph, and the python code itself. Each of these domains have been researched independently. Document representations have been a long researched area within NLP, graph representations are becoming a hotter area of research, code representations are a newer but largely unexplored area of representation learning. This dataset not only provides a new context to further explore these domains, but allows for researchers to explore the interaction and generalization of methods that may apply across all of these domains. 

One can imagine that sentence can be thought of a random walk through a language graph thus, the strong performance of word2vec \cite{word2vec} and other neural probabilistic language models \cite{bengio2003neural} can be applied in an effective manner on arbitrary graphs to learn node representations \cite{deepwalk}. We argue that this analogy can and should be pushed further in the pursuit of building general cross-domain methods. 

Within the PyPI dataset we interpret each package as a graph consisting of a subgraph for each domain. As shown in [ADD A FIGURE AND REFERENCE HERE] a package graph consists of a language graph, a dependency graph, and a code graph. The following sections detail how we construct each subgraph and the current SOTA methods in representation learning for that domain.

\subsection{The Language Graph}

The language graph consists of either the package's GitHub README file or its PyPI description (whichever is longer). If we were to learn a python package representation from just this graph we would end up with a reasonable measure of package similarity as a package creator wants to explain what the package features. However, we reiterate that this one domain is a part to the whole concept that is a python package. [NEED TRANSITION]. We discuss the following methods that learn document representations: TF-IDF and Doc2Vec \cite{doc2vec}.

 TF-IDF builds vectors of length of a keyword dictionary. At the corresponding keyword’s index there is a value in the range [0, 1] indicating the relative importance of the term to that python package’s documentation. The main advantage of this approach is also the interpretability of the calculated vectors, but also encodes more information than the Binary Keyword approach. These vectors were calculated with sklearn’s TFIDFVectorizer class.
 
 Doc2Vec \cite{doc2vec} learns a dense representation of documents alongside a vocabulary. This extends the word2vec \cite{word2vec} approach to documents. This approach has a much better memory requirement as we don’t need each vector to be the length of a keyword dictionary. These word2vec type approaches are well known for encoding the semantic relationships of language.

\subsection{The Direct Dependency Graph}

\subsection{The Code Graph}

\subsection{The Cross Domain Graph}
Our maximum likelihood w.r.t the parameters of our model across all domains present becomes:
\begin{equation}
\arg\max_{\theta} \prod_{i}^{D}\prod_{t}^{V}P(v^{(i)}_{t} | v^{(i)}_{-w:w}, \mathbf{d}; \theta)
\end{equation}
Where $D$ is the set of all domains, $v^{(i)}_{t}$ is the $t$th vertex in the $i$th domain, and $mathbf{d}$ is the document vector.

\section{Representation Evaluation}

\subsection{Topic Classification}
In order to evaluate the learned python package embeddings we use an extrinsic evaluation task. We define this task as the classification of python packages into different categories. Github allows for users to label their projects under various tags - as a vast majority of PyPI packages link to a github we were able to extract a subset of python packages that are labeled by the creator into these categories. Following the example of Perozzi et al. (maybe add others?) we use a logistic regression and a shallow neural network to classify the learned representations.

This task empirically shows that no single domain is sufficient to capture a complete representation of a python package.

\subsection{Link Prediction}

\subsection{Topic Generation}

\section{Conclusion}





% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{doc2vec}

\bibliography{example_paper}
\bibliographystyle{icml2019}

\end{document}
