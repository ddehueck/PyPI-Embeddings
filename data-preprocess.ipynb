{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook turns the data we retrieved in the data retrieval notebook into a graph format. \n",
    "Will end up with:\n",
    "\n",
    "1. **pypi_nodes.csv** - Nodes with an index/id and the name from pypi\n",
    "2. **pypi_edges.csv** - Edges between node ids - found via GitHub dependency graphs\n",
    "3. **pypi_nodes_lang.csv** - Language e.g. READMEs affiliated with each node - indexed by node\n",
    "4. **eval_topics.csv** - Top 200 topics associated with packages with GitHub repos\n",
    "5. **node_topics.csv** - Nodes with assigned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the data from PyPI\n",
    "pypi_data_path = 'data_retrieval/data/pypi_data.json'\n",
    "\n",
    "# Data from GitHub API for packages with github links\n",
    "github_data_path = 'data_retrieval/data/github_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ujson\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_preprocess.github_data import GitHub\n",
    "from data_preprocess.graph_data import PyPIGraph\n",
    "from data_preprocess.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyPI:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodes_path = 'data_preprocess/data/pypi_nodes.csv'\n",
    "        self.edges_path = 'data_preprocess/data/pypi_edges.csv'\n",
    "        self.lang_path = 'data_preprocess/data/pypi_nodes_lang.csv'\n",
    "\n",
    "        self.github_data = GitHub(github_data_path)\n",
    "        self.eval_topics_path = 'data_preprocess/data/eval_topics.csv'\n",
    "        self.node_topics_path = 'data_preprocess/data/node_topics.csv'\n",
    "\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "        with open(pypi_data_path, 'r', encoding='utf-8') as f:\n",
    "            self.saved_data = ujson.load(f)\n",
    "\n",
    "        self.saved_data_name_dict = dict((d['name'], d) for d in self.saved_data['data'])\n",
    "\n",
    "    def create_eval_topics_list(self):\n",
    "        eval_topics = self.github_data.get_evaluation_topics(n_top=200)\n",
    "\n",
    "        eval_topics_df = pd.DataFrame(eval_topics, columns=[\"topics\"])\n",
    "        print(eval_topics_df)\n",
    "        eval_topics_df.to_csv(self.eval_topics_path)\n",
    "        print('Saved Evaluation Topics!')\n",
    "\n",
    "    def create_node_topics_list(self):\n",
    "        eval_topics = self.get_eval_topics()\n",
    "        nodes = self.get_nodes()\n",
    "\n",
    "        node_topic = []\n",
    "        for pkg, data in self.github_data.pkgs_with_topics_gen():\n",
    "            for t in data['topics']:\n",
    "                if t['name'] in eval_topics:\n",
    "                    node_id = nodes.index(pkg)\n",
    "                    topic_id = eval_topics.index(t['name'])\n",
    "                    node_topic.append([node_id, topic_id])\n",
    "\n",
    "        node_topic_df = pd.DataFrame(node_topic, columns=[\"node_id\", \"topic_id\"])\n",
    "        print(node_topic_df)\n",
    "        node_topic_df.to_csv(self.node_topics_path)\n",
    "        print('Saved Node Topics List!')\n",
    "\n",
    "    def create_node_language_list(self):\n",
    "        nodes = self.get_nodes()\n",
    "        nodes_lang = []\n",
    "\n",
    "        print('Creating node language list (tokenizing too)...')\n",
    "        for node in tqdm(nodes):\n",
    "            # Use GitHub README or PyPI Description\n",
    "            readme = self.github_data.get_readme(node)\n",
    "            desc = self.saved_data_name_dict.get(node)['description']\n",
    "            desc = desc if desc != 'UNKNOWN' else ''\n",
    "            # Only take longest language value - avoid repeats\n",
    "            tokenized = self.tokenizer.tokenize_doc(max(readme, desc))\n",
    "            nodes_lang.append(' '.join(tokenized))  # Join so one line\n",
    "\n",
    "        assert len(nodes_lang) == len(nodes)\n",
    "        nodes_lang_df = pd.DataFrame(nodes_lang, columns=[\"language\"])\n",
    "        print(nodes_lang_df)\n",
    "        nodes_lang_df.to_csv(self.lang_path)\n",
    "        print('Saved PyPI Node Language!')\n",
    "\n",
    "    def create_edge_list(self):\n",
    "        pkg_deps_dict = self.github_data.get_pkg_dependency_dict()\n",
    "        nodes = self.get_nodes()\n",
    "        edges = []\n",
    "\n",
    "        print(\"Creating edge list...\")\n",
    "        for pkg in tqdm(pkg_deps_dict):\n",
    "            for dep in pkg_deps_dict[pkg]:\n",
    "                # Ensure we have a node record\n",
    "                if pkg in nodes and dep in nodes:\n",
    "                    pkg_id, dep_id = str(nodes.index(pkg)), str(nodes.index(dep))\n",
    "                    # Undirected!\n",
    "                    edges.append([pkg_id, dep_id])\n",
    "                    edges.append([dep_id, pkg_id])\n",
    "\n",
    "        # Save edge list to a CSV\n",
    "        edges_df = pd.DataFrame(edges, columns=[\"src\", \"dest\"])\n",
    "        print(edges_df)\n",
    "        edges_df.to_csv(self.edges_path)\n",
    "        print('Saved PyPI Edges!')\n",
    "\n",
    "    def create_nodes(self):\n",
    "        print(\"Creating Nodes...\")\n",
    "        nodes = [str(pkg[\"name\"]) for pkg in self.saved_data['data']]\n",
    "\n",
    "        # Save nodes to a CSV\n",
    "        nodes_df = pd.DataFrame(nodes, columns=[\"nodes\"])\n",
    "        print(nodes_df)\n",
    "        nodes_df.to_csv(self.nodes_path)\n",
    "        print('Saved PyPI Nodes!')\n",
    "\n",
    "    def get_nodes(self):\n",
    "        if os.path.exists(self.nodes_path):\n",
    "            return list(pd.read_csv(self.nodes_path, na_filter=False)[\"nodes\"])\n",
    "        raise NotImplementedError(\"There are no saved nodes - call create_nodes()!\")\n",
    "\n",
    "    def get_eval_topics(self):\n",
    "        if os.path.exists(self.eval_topics_path):\n",
    "            return list(pd.read_csv(self.eval_topics_path)[\"topics\"])\n",
    "        raise NotImplementedError(\"There are no saved nodes - call create_eval_topics_list()!\")\n",
    "\n",
    "    def print_statistics(self):\n",
    "        edges = pd.read_csv(self.edges_path).values\n",
    "        nodes = self.get_nodes()\n",
    "        print('\\n------------------------------')\n",
    "        print('     PyPI Data Statistics    ')\n",
    "        print('------------------------------')\n",
    "\n",
    "        print(f\"Total Number of Nodes: {len(nodes)}\")\n",
    "        print(f\"Total Number of Edges: {len(edges)}\")\n",
    "\n",
    "        graph = PyPIGraph(self.nodes_path, self.edges_path, self.lang_path)\n",
    "        num_nodes_with_connections = graph.num_nodes_with_connections()\n",
    "        print(f'Total of {num_nodes_with_connections}/{len(nodes)} has connections')\n",
    "\n",
    "        num_node_lang = graph.num_nodes_with_features()\n",
    "        print(f\"Total Number of Nodes with language data: {num_node_lang}\")\n",
    "\n",
    "        num_nodes_graph_and_lang_data = graph.num_nodes_with_connections_and_features()\n",
    "        print(f'Total of {num_nodes_graph_and_lang_data} have language and graph data')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5ef141629ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-20c98c7ce33d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_topics_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data_preprocess/data/node_topics.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpypi_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/experiments/project2vec/jupyter_notebooks/data_preprocess/tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, custom_stop, merge_noun_chunks)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Define pipeline - use different nlp is using pretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Use for unique vocabularies - i.e. Hacker News Experiement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerge_noun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "data = PyPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Nodes...\n",
      "                    nodes\n",
      "0                 sizzler\n",
      "1         django-mongokit\n",
      "2        gmaps-url-parser\n",
      "3   django-excel-response\n",
      "4                  slidoc\n",
      "..                    ...\n",
      "93           django-elmah\n",
      "94                mambupy\n",
      "95           django-jauth\n",
      "96                  awsme\n",
      "97               yakstack\n",
      "\n",
      "[98 rows x 1 columns]\n",
      "Saved PyPI Nodes!\n"
     ]
    }
   ],
   "source": [
    "# Create nodes - assign index/id and name to node\n",
    "data.create_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 15293.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating edge list...\n",
      "  src dest\n",
      "0  87   87\n",
      "1  87   87\n",
      "Saved PyPI Edges!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a src -> dest edge list between node ids\n",
    "# Undirected so two connections per link\n",
    "data.create_edge_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/98 [00:00<00:10,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating node language list (tokenizing too)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:14<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             language\n",
      "0   sizzler vpn websocket sizzler linux tool set v...\n",
      "1   django mongokit peter bengtsson 2010 2011 lice...\n",
      "2   gmaps url parser |build| |downloads| |license|...\n",
      "3   django excel response image target alt late ve...\n",
      "4   slidoc image target image target image target ...\n",
      "..                                                ...\n",
      "93  djelmah djelmah catch unhandled exception prod...\n",
      "94  image target alt build status mambupy python a...\n",
      "95  django jauth simple oauth2 authentication clie...\n",
      "96  amazon web services cloud watch metrics librar...\n",
      "97  yakstack command line utility help stack yak e...\n",
      "\n",
      "[98 rows x 1 columns]\n",
      "Saved PyPI Node Language!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all documents and index by node id\n",
    "# May take hours for all data\n",
    "data.create_node_language_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 93 topics to chose from\n",
      "Getting top 200 topics\n",
      "             topics\n",
      "0            django\n",
      "1             excel\n",
      "2               csv\n",
      "3          maildrop\n",
      "4           testing\n",
      "..              ...\n",
      "83  static-analysis\n",
      "84  static-analyzer\n",
      "85        simulator\n",
      "86       api-client\n",
      "87         requests\n",
      "\n",
      "[88 rows x 1 columns]\n",
      "Saved Evaluation Topics!\n"
     ]
    }
   ],
   "source": [
    "# Get up to top 200 topics from GitHub repos\n",
    "# Assign an index to each topic\n",
    "data.create_eval_topics_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    node_id  topic_id\n",
      "0         3         0\n",
      "1         3         1\n",
      "2         3         2\n",
      "3         6         3\n",
      "4         6         4\n",
      "..      ...       ...\n",
      "84       86        83\n",
      "85       86        84\n",
      "86       86        85\n",
      "87       90        86\n",
      "88       90        87\n",
      "\n",
      "[89 rows x 2 columns]\n",
      "Saved Node Topics List!\n"
     ]
    }
   ],
   "source": [
    "# Label a node as belonging to topics\n",
    "data.create_node_topics_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
