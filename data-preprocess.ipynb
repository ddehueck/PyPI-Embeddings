{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook turns the data we retrieved in the data retrieval notebook into a graph format. \n",
    "Will end up with:\n",
    "\n",
    "1. **pypi_nodes.csv** - Nodes with an index/id and the name from pypi\n",
    "2. **pypi_edges.csv** - Edges between node ids - found via GitHub dependency graphs\n",
    "3. **pypi_nodes_lang.csv** - Language e.g. READMEs affiliated with each node - indexed by node\n",
    "4. **eval_topics.csv** - Top 200 topics associated with packages with GitHub repos\n",
    "5. **node_topics.csv** - Nodes with assigned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the data from PyPI\n",
    "pypi_data_path = 'retrieved_data/pypi_data.json'\n",
    "\n",
    "# Only Packages from PyPI that linked to Github Repo\n",
    "pypi_github_pkgs_path = 'retrieved_data/pypi_github_data.json'\n",
    "\n",
    "# Data from GitHub API for packages identified in step 2\n",
    "github_data_path = 'retrieved_data/github_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ujson\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from github_data import GitHub\n",
    "from graph_data import PyPIGraph\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyPI:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nodes_path = 'pypi_nodes.csv'\n",
    "        self.edges_path = 'pypi_edges.csv'\n",
    "        self.lang_path = 'pypi_nodes_lang.csv'\n",
    "\n",
    "        self.github_data = GitHub(github_data_path)\n",
    "        self.eval_topics_path = 'eval_topics.csv'\n",
    "        self.node_topics_path = 'node_topics.csv'\n",
    "\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "        with open(pypi_data_path, 'r', encoding='utf-8') as f:\n",
    "            self.saved_data = ujson.load(f)\n",
    "\n",
    "        self.saved_data_name_dict = dict((d['name'], d) for d in self.saved_data['data'])\n",
    "\n",
    "    def create_eval_topics_list(self):\n",
    "        eval_topics = self.github_data.get_evaluation_topics(n_top=200)\n",
    "\n",
    "        eval_topics_df = pd.DataFrame(eval_topics, columns=[\"topics\"])\n",
    "        print(eval_topics_df)\n",
    "        eval_topics_df.to_csv(self.eval_topics_path)\n",
    "        print('Saved Evaluation Topics!')\n",
    "\n",
    "    def create_node_topics_list(self):\n",
    "        eval_topics = self.get_eval_topics()\n",
    "        nodes = self.get_nodes()\n",
    "\n",
    "        node_topic = []\n",
    "        for pkg, data in self.github_data.pkgs_with_topics_gen():\n",
    "            for t in data['topics']:\n",
    "                if t['name'] in eval_topics:\n",
    "                    node_id = nodes.index(pkg)\n",
    "                    topic_id = eval_topics.index(t['name'])\n",
    "                    node_topic.append([node_id, topic_id])\n",
    "\n",
    "        node_topic_df = pd.DataFrame(node_topic, columns=[\"node_id\", \"topic_id\"])\n",
    "        print(node_topic_df)\n",
    "        node_topic_df.to_csv(self.node_topics_path)\n",
    "        print('Saved Node Topics List!')\n",
    "\n",
    "    def create_node_language_list(self):\n",
    "        nodes = self.get_nodes()\n",
    "        nodes_lang = []\n",
    "\n",
    "        print('Creating node language list (tokenizing too)...')\n",
    "        for node in tqdm(nodes):\n",
    "            # Use GitHub README or PyPI Description\n",
    "            readme = self.github_data.get_readme(node)\n",
    "            desc = self.saved_data_name_dict.get(node)['description']\n",
    "            desc = desc if desc != 'UNKNOWN' else ''\n",
    "            # Only take longest language value - avoid repeats\n",
    "            tokenized = self.tokenizer.tokenize_doc(max(readme, desc))\n",
    "            nodes_lang.append(' '.join(tokenized))  # Join so one line\n",
    "\n",
    "        assert len(nodes_lang) == len(nodes)\n",
    "        nodes_lang_df = pd.DataFrame(nodes_lang, columns=[\"language\"])\n",
    "        print(nodes_lang_df)\n",
    "        nodes_lang_df.to_csv(self.lang_path)\n",
    "        print('Saved PyPI Node Language!')\n",
    "\n",
    "    def create_edge_list(self):\n",
    "        pkg_deps_dict = self.github_data.get_pkg_dependency_dict()\n",
    "        nodes = self.get_nodes()\n",
    "        edges = []\n",
    "\n",
    "        print(\"Creating edge list...\")\n",
    "        for pkg in tqdm(pkg_deps_dict):\n",
    "            for dep in pkg_deps_dict[pkg]:\n",
    "                # Ensure we have a node record\n",
    "                if pkg in nodes and dep in nodes:\n",
    "                    pkg_id, dep_id = str(nodes.index(pkg)), str(nodes.index(dep))\n",
    "                    # Undirected!\n",
    "                    edges.append([pkg_id, dep_id])\n",
    "                    edges.append([dep_id, pkg_id])\n",
    "\n",
    "        # Save edge list to a CSV\n",
    "        edges_df = pd.DataFrame(edges, columns=[\"src\", \"dest\"])\n",
    "        print(edges_df)\n",
    "        edges_df.to_csv(self.edges_path)\n",
    "        print('Saved PyPI Edges!')\n",
    "\n",
    "    def create_nodes(self):\n",
    "        print(\"Creating Nodes...\")\n",
    "        nodes = [str(pkg[\"name\"]) for pkg in self.saved_data['data']]\n",
    "\n",
    "        # Save nodes to a CSV\n",
    "        nodes_df = pd.DataFrame(nodes, columns=[\"nodes\"])\n",
    "        print(nodes_df)\n",
    "        nodes_df.to_csv(self.nodes_path)\n",
    "        print('Saved PyPI Nodes!')\n",
    "\n",
    "    def get_nodes(self):\n",
    "        if os.path.exists(self.nodes_path):\n",
    "            return list(pd.read_csv(self.nodes_path, na_filter=False)[\"nodes\"])\n",
    "        raise NotImplementedError(\"There are no saved nodes - call create_nodes()!\")\n",
    "\n",
    "    def get_eval_topics(self):\n",
    "        if os.path.exists(self.eval_topics_path):\n",
    "            return list(pd.read_csv(self.eval_topics_path)[\"topics\"])\n",
    "        raise NotImplementedError(\"There are no saved nodes - call create_eval_topics_list()!\")\n",
    "\n",
    "    def print_statistics(self):\n",
    "        edges = pd.read_csv(self.edges_path).values\n",
    "        nodes = self.get_nodes()\n",
    "        print('\\n------------------------------')\n",
    "        print('     PyPI Data Statistics    ')\n",
    "        print('------------------------------')\n",
    "\n",
    "        print(f\"Total Number of Nodes: {len(nodes)}\")\n",
    "        print(f\"Total Number of Edges: {len(edges)}\")\n",
    "\n",
    "        graph = PyPIGraph('pypi_nodes.csv', 'pypi_edges.csv', 'pypi_nodes_lang.csv')\n",
    "        num_nodes_with_connections = graph.num_nodes_with_connections()\n",
    "        print(f'Total of {num_nodes_with_connections}/{len(nodes)} has connections')\n",
    "\n",
    "        num_node_lang = graph.num_nodes_with_features()\n",
    "        print(f\"Total Number of Nodes with language data: {num_node_lang}\")\n",
    "\n",
    "        num_nodes_graph_and_lang_data = graph.num_nodes_with_connections_and_features()\n",
    "        print(f'Total of {num_nodes_graph_and_lang_data} have language and graph data')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PyPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Nodes...\n",
      "                   nodes\n",
      "0              mediajson\n",
      "1                alengen\n",
      "2        chunked-scatter\n",
      "3     sphinxcontrib-nvd3\n",
      "4                 pyprot\n",
      "..                   ...\n",
      "92                 fanyi\n",
      "93          localstorage\n",
      "94                xy-tel\n",
      "95  django-custom-mixins\n",
      "96             pycleaner\n",
      "\n",
      "[97 rows x 1 columns]\n",
      "Saved PyPI Nodes!\n"
     ]
    }
   ],
   "source": [
    "# Create nodes - assign index/id and name to node\n",
    "data.create_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 52675.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating edge list...\n",
      "  src dest\n",
      "0  28   28\n",
      "1  28   28\n",
      "2  53   53\n",
      "3  53   53\n",
      "4  58   58\n",
      "5  58   58\n",
      "6  80   80\n",
      "7  80   80\n",
      "Saved PyPI Edges!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a src -> dest edge list between node ids\n",
    "# Undirected so two connections per link\n",
    "data.create_edge_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating node language list (tokenizing too)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:10<00:00,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             language\n",
      "0   json serialiser parser python support extensio...\n",
      "1   alengen generate model table sqlalchemy port p...\n",
      "2   chunked_scatter tool take bed file sequence di...\n",
      "3   sphinxcontrib.nvd3 sphinx chart extension nvd3...\n",
      "4   package design represent maniupate amino acid ...\n",
      "..                                                ...\n",
      "92                                                   \n",
      "93  localstorage image target image target image t...\n",
      "94                                 car brand juhe api\n",
      "95  django custom mixin list custom mixin project ...\n",
      "96                                                   \n",
      "\n",
      "[97 rows x 1 columns]\n",
      "Saved PyPI Node Language!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all documents and index by node id\n",
    "# May take hours for all data\n",
    "data.create_node_language_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 60 topics to chose from\n",
      "Getting top 200 topics\n",
      "                     topics\n",
      "0                    django\n",
      "1                rd-project\n",
      "2           rd-section-apmm\n",
      "3                   protein\n",
      "4                 alignment\n",
      "5               amino-acids\n",
      "6             substitutions\n",
      "7              score-matrix\n",
      "8                 profiling\n",
      "9      structure-prediction\n",
      "10           cryptocurrency\n",
      "11                   crypto\n",
      "12         cryptocurrencies\n",
      "13                     news\n",
      "14            cryptocontrol\n",
      "15                      api\n",
      "16               api-client\n",
      "17                      zim\n",
      "18                  openzim\n",
      "19                  youtube\n",
      "20                  scraper\n",
      "21          django-packages\n",
      "22                      orm\n",
      "23                 database\n",
      "24                  awesome\n",
      "25          database-schema\n",
      "26               torrent-dl\n",
      "27                 peerflix\n",
      "28                   pyflix\n",
      "29                  torrent\n",
      "30                   magnet\n",
      "31                      vlc\n",
      "32                   stream\n",
      "33                  remmina\n",
      "34  remmina-rdp-connections\n",
      "35         network-analysis\n",
      "36               geospatial\n",
      "37                   meteor\n",
      "38            collaboration\n",
      "39                 topogram\n",
      "40               tensorflow\n",
      "41         machine-learning\n",
      "42              apache-beam\n",
      "43                   jinja2\n",
      "44         jinja2-templates\n",
      "45                   devops\n",
      "46          developer-tools\n",
      "47             many-to-many\n",
      "48               django-orm\n",
      "49                   toggle\n",
      "50       radiative-transfer\n",
      "51               exoplanets\n",
      "52             spectroscopy\n",
      "53                   hitran\n",
      "54                   exomol\n",
      "Saved Evaluation Topics!\n"
     ]
    }
   ],
   "source": [
    "# Get up to top 200 topics from GitHub repos\n",
    "# Assign an index to each topic\n",
    "data.create_eval_topics_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    node_id  topic_id\n",
      "0         0         1\n",
      "1         0         2\n",
      "2         4         3\n",
      "3         4         4\n",
      "4         4         5\n",
      "5         4         6\n",
      "6         4         7\n",
      "7         4         8\n",
      "8         4         9\n",
      "9         6        10\n",
      "10        6        11\n",
      "11        6        12\n",
      "12        6        13\n",
      "13        6        14\n",
      "14        6        15\n",
      "15        6        16\n",
      "16        7        17\n",
      "17        7        18\n",
      "18        7        19\n",
      "19        7        20\n",
      "20       29         0\n",
      "21       29        21\n",
      "22       29        22\n",
      "23       29        23\n",
      "24       29        24\n",
      "25       29        25\n",
      "26       30        26\n",
      "27       30        27\n",
      "28       30        28\n",
      "29       30        29\n",
      "30       30        30\n",
      "31       30        31\n",
      "32       30        32\n",
      "33       41        33\n",
      "34       41        34\n",
      "35       44        35\n",
      "36       44        36\n",
      "37       44        37\n",
      "38       44        38\n",
      "39       44        39\n",
      "40       58        40\n",
      "41       58        41\n",
      "42       58        42\n",
      "43       65        43\n",
      "44       65        44\n",
      "45       65        45\n",
      "46       65        46\n",
      "47       66         0\n",
      "48       66        47\n",
      "49       66        48\n",
      "50       66        49\n",
      "51       79        50\n",
      "52       79        51\n",
      "53       79        52\n",
      "54       79        53\n",
      "55       79        54\n",
      "Saved Node Topics List!\n"
     ]
    }
   ],
   "source": [
    "# Label a node as belonging to topics\n",
    "data.create_node_topics_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "     PyPI Data Statistics    \n",
      "------------------------------\n",
      "Total Number of Nodes: 97\n",
      "Total Number of Edges: 8\n",
      "Total of 4/97 has connections\n",
      "Total Number of Nodes with language data: 84\n",
      "Total of 4 have language and graph data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
